{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# core\n",
    "\n",
    "> The core classes and functionality for SentenceGraph."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| default_exp core"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| hide\n",
    "from nbdev.showdoc import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#|export\n",
    "from sentence_transformers import SentenceTransformer\n",
    "from sentence_transformers.util import cos_sim\n",
    "from typing import List\n",
    "from enum import Enum\n",
    "import pandas as pd\n",
    "from collections import namedtuple\n",
    "import numpy as np\n",
    "import random as random\n",
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "class Format(Enum):\n",
    "  Python = 0\n",
    "  Torch = 1\n",
    "  Numpy = 2\n",
    "  Pandas = 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "TextNode = namedtuple('TextNodeTuple', 'id text')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'Format' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn [1], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[39m#| export\u001b[39;00m\n\u001b[0;32m----> 2\u001b[0m \u001b[39mclass\u001b[39;00m \u001b[39mSentenceGraph\u001b[39;00m:\n\u001b[1;32m      3\u001b[0m     \u001b[39mdef\u001b[39;00m \u001b[39m__init__\u001b[39m(\u001b[39mself\u001b[39m, model_name: \u001b[39mstr\u001b[39m \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m, model: \u001b[39mNone\u001b[39;00m \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m):\n\u001b[1;32m      4\u001b[0m         \u001b[39mif\u001b[39;00m model_name \u001b[39m==\u001b[39m \u001b[39mNone\u001b[39;00m \u001b[39mor\u001b[39;00m model_name \u001b[39m==\u001b[39m \u001b[39m\"\u001b[39m\u001b[39m\"\u001b[39m \u001b[39mor\u001b[39;00m model \u001b[39m==\u001b[39m \u001b[39mNone\u001b[39;00m:\n",
      "Cell \u001b[0;32mIn [1], line 11\u001b[0m, in \u001b[0;36mSentenceGraph\u001b[0;34m()\u001b[0m\n\u001b[1;32m      8\u001b[0m     \u001b[39melif\u001b[39;00m model_name \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m      9\u001b[0m       \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mmodel \u001b[39m=\u001b[39m model \u001b[39m=\u001b[39m SentenceTransformer(model_name)\n\u001b[0;32m---> 11\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mcreateGraph\u001b[39m(\u001b[39mself\u001b[39m, sentences: List[TextNode], \u001b[39mformat\u001b[39m: Format \u001b[39m=\u001b[39m Format\u001b[39m.\u001b[39mPython) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m List[List[\u001b[39mfloat\u001b[39m]]:\n\u001b[1;32m     12\u001b[0m     \u001b[39m# TODO: Wrap this all in a function to export a functional version of this as well.\u001b[39;00m\n\u001b[1;32m     13\u001b[0m     sentence_embeddings \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mmodel\u001b[39m.\u001b[39mencode([node\u001b[39m.\u001b[39mnodeText \u001b[39mfor\u001b[39;00m node \u001b[39min\u001b[39;00m sentences])\n\u001b[1;32m     15\u001b[0m     graph_len \u001b[39m=\u001b[39m \u001b[39mlen\u001b[39m(sentence_embeddings)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'Format' is not defined"
     ]
    }
   ],
   "source": [
    "#| export\n",
    "class SentenceGraph:\n",
    "    def __init__(self, model_name: str = None, model: None = None):\n",
    "        if model_name == None or model_name == \"\" or model == None:\n",
    "            self.model = model = SentenceTransformer('all-MiniLM-L6-v2')\n",
    "        elif model is not None:\n",
    "            self.model = model\n",
    "        elif model_name is not None:\n",
    "          self.model = model = SentenceTransformer(model_name)\n",
    "\n",
    "    def create_graph(self, sentences: List[TextNode], format: Format = Format.Python) -> List[List[float]]:\n",
    "        # TODO: Wrap this all in a function to export a functional version of this as well.\n",
    "        sentence_embeddings = self.model.encode([node.text for node in sentences])\n",
    "\n",
    "        graph_len = len(sentence_embeddings)\n",
    "        similarity_graph = [[0 for x in range(graph_len)] for y in range(graph_len)]\n",
    "\n",
    "        for i, embeddingA in enumerate(sentence_embeddings):\n",
    "          for j, embeddingB in enumerate(sentence_embeddings):\n",
    "            similarity = cos_sim(embeddingA, embeddingB)\n",
    "\n",
    "            similarity = similarity.tolist()[0][0]\n",
    "\n",
    "            similarity_graph[i][j] = similarity\n",
    "        \n",
    "        if format == Format.Pandas:\n",
    "          index = [node.id for node in sentences]\n",
    "          df = pd.DataFrame(similarity_graph, index = index,\n",
    "                                          columns = index)\n",
    "          return df\n",
    "        elif format == Format.Numpy:\n",
    "          return np.array(similarity_graph)\n",
    "        elif format == Format.Torch:\n",
    "          return torch.asarray(similarity_graph)\n",
    "        else:\n",
    "          return similarity_graph\n",
    "\n",
    "    def _select_top_edges(self, sim_matrix, extract_num, beta, lambda1, lambda2) -> List[int]:\n",
    "        \"\"\" \n",
    "          Code taken from: https://github.com/mswellhao/PacSum\n",
    "          Given a similiarity matrix of text edge scores. Select the most semantically relevant edges.\n",
    "        \"\"\"\n",
    "        # Compute the edge threshold and new edge \n",
    "        min_score = sim_matrix.min()\n",
    "        max_score = sim_matrix.max()\n",
    "        edge_threshold = min_score + beta * (max_score - min_score)\n",
    "        new_edge_scores = sim_matrix - edge_threshold\n",
    "        forward_scores, backward_scores, _ = self._compute_new_edge_scores(new_edge_scores)\n",
    "        forward_scores = 0 - forward_scores\n",
    "\n",
    "        paired_scores = []\n",
    "        for node in range(len(forward_scores)):\n",
    "            paired_scores.append([node,  lambda1 * forward_scores[node] + lambda2 * backward_scores[node]])\n",
    "\n",
    "        #shuffle to avoid any possible bias\n",
    "        random.shuffle(paired_scores)\n",
    "        paired_scores.sort(key = lambda x: x[1], reverse = True)\n",
    "        extracted = [item[0] for item in paired_scores[:extract_num]]\n",
    "\n",
    "        return extracted\n",
    "\n",
    "    def _compute_new_edge_scores(self, similarity_matrix, edge_threshold = 0):\n",
    "        \"\"\" \n",
    "          Code taken from https://github.com/mswellhao/PacSum \n",
    "          Grabs the most important edges based off of an edge threshold\n",
    "        \"\"\"\n",
    "        forward_scores = [0 for i in range(len(similarity_matrix))]\n",
    "        backward_scores = [0 for i in range(len(similarity_matrix))]\n",
    "        edges = []\n",
    "        for i in range(len(similarity_matrix)):\n",
    "            for j in range(i+1, len(similarity_matrix[i])):\n",
    "                edge_score = similarity_matrix[i][j]\n",
    "            if edge_score > edge_threshold:\n",
    "                forward_scores[j] += edge_score\n",
    "                backward_scores[i] += edge_score\n",
    "                edges.append((i,j,edge_score))\n",
    "\n",
    "        return np.asarray(forward_scores), np.asarray(backward_scores), edges\n",
    "\n",
    "    def extract_summary(self, textNodes: List[TextNode], num_nodes = 3, beta = 3, lambda1 = -0.2, lambda2 = -0.2) -> str:\n",
    "        \"\"\" \n",
    "          Given a list of text, create an extractive summary with the top n nodes.\n",
    "          Beta, lambda1, and lambda2 come from the https://github.com/mswellhao/PacSum paper along with the current extractor algorithim.\n",
    "        \"\"\"\n",
    "        sim_matrix = self.create_graph(textNodes, format=Format.Numpy)\n",
    "\n",
    "        top_sentences = self._select_top_edges(sim_matrix, num_nodes, beta, lambda1, lambda2)\n",
    "        summary = [textNodes[idx].text for idx in top_sentences]\n",
    "        return ' '.join(summary)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| hide\n",
    "import nbdev; nbdev.nbdev_export()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
